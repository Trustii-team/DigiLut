{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0577e13-68fb-47bc-81d0-441d7217cc9d",
   "metadata": {},
   "source": [
    "### Imports and utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaad8a8",
   "metadata": {},
   "source": [
    "Ensure that OpenSlide is installed. If you are using the provided environment, it should already be installed. To set up the environment and download the necessary packages, please follow the steps outlined in the README and install.txt files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53395884-3b84-4297-b854-906421b566cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openslide-python\n",
      "  Using cached openslide_python-1.3.1-cp39-cp39-linux_x86_64.whl\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.9/site-packages (from openslide-python) (9.1.0)\n",
      "Installing collected packages: openslide-python\n",
      "Successfully installed openslide-python-1.3.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install openslide-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b695225",
   "metadata": {},
   "source": [
    "Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48bafc33-0d6a-4466-9852-c7df31013463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openslide \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import cv2 \n",
    "from scipy.stats import scoreatpercentile\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm \n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import zipfile \n",
    "import tempfile \n",
    "from skimage import io, color\n",
    "from skimage.exposure import is_low_contrast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9492fbf3",
   "metadata": {},
   "source": [
    "Adjust the folder_path as needed to match your directory. The images should be placed in the specified folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9b14566-340b-4717-b646-7a52317900a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = './data/images'\n",
    "level = 2\n",
    "patch_size=224\n",
    "step_size=200\n",
    "patch_file_name='more_selective_patches_7'\n",
    "patches_path=f'./patches_csv/{patch_file_name}_level_{level}_size_{patch_size}_step_{step_size}.csv'\n",
    "csv_dir = f'./patches_csv/{patch_file_name}_level_{level}_size_{patch_size}_step_{step_size}'\n",
    "csv_file=f'./patches_csv/{patch_file_name}_level_{level}_size_{patch_size}_step_{step_size}.csv'\n",
    "zip_name = f\"{patch_file_name}_level_{level}_size_{patch_size}_step_{step_size}.zip\"\n",
    "\n",
    "# Ensure the directory patches_csv and data exists as well as the image folder.\n",
    "os.makedirs(os.path.dirname(patches_path), exist_ok=True)\n",
    "os.makedirs(folder_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b62f87",
   "metadata": {},
   "source": [
    "## Preprocessing Functions\n",
    "The following functions are provided for preprocessing whole slide images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d53c8181-c824-4559-a105-8ebf0c596bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_images(slide, x, y, width, height, level):\n",
    "    \"\"\"\n",
    "    Scales the coordinates and dimensions of a patch to the base level (level 0) of a whole slide image.\n",
    "\n",
    "    Parameters:\n",
    "    slide (openslide.OpenSlide): An OpenSlide object representing the whole slide image.\n",
    "    x (int): The x-coordinate of the top-left corner of the patch at the specified level.\n",
    "    y (int): The y-coordinate of the top-left corner of the patch at the specified level.\n",
    "    width (int): The width of the patch at the specified level.\n",
    "    height (int): The height of the patch at the specified level.\n",
    "    level (int): The magnification level (0-5, where 0 is the base level and 5 is the most zoomed-out).\n",
    "\n",
    "    Returns:\n",
    "    tuple: Scaled coordinates and dimensions (base_x, base_y, base_width, base_height) at the base level.\n",
    "    \"\"\"\n",
    " \n",
    "    scale_factor = slide.level_downsamples[level]\n",
    "    base_x = int(x * scale_factor)\n",
    "    base_y = int(y * scale_factor)\n",
    "    base_width = int(width * scale_factor)\n",
    "    base_height = int(height * scale_factor)\n",
    "    return base_x, base_y, base_width, base_height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22fe6d3d-17cd-4ca9-947f-c9145be8e721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_patch_not_within_bounds(slide, base_width, base_height, base_x, base_y):\n",
    "    \"\"\"\n",
    "    Checks if the requested patch is within the bounds of the base level (level 0) of a whole slide image.\n",
    "\n",
    "    Parameters:\n",
    "    slide (openslide.OpenSlide): An OpenSlide object representing the whole slide image.\n",
    "    base_height (int): The height of the patch at the base level.\n",
    "    base_width (int): The width of the patch at the base level.\n",
    "    base_x (int): The x-coordinate of the top-left corner of the patch at the base level.\n",
    "    base_y (int): The y-coordinate of the top-left corner of the patch at the base level.\n",
    "\n",
    "    Returns:\n",
    "    int: Returns 1 if the patch is out of bounds, otherwise returns 0.\n",
    "    \"\"\"\n",
    "    base_dims = slide.level_dimensions[0]\n",
    "    if base_x + base_width > base_dims[0] or base_y + base_height > base_dims[1]:\n",
    "        print('x', base_x + base_width, base_dims[0])\n",
    "        print('y', base_y + base_height, base_dims[1])\n",
    "        print(\"Requested patch is out of bounds.\")\n",
    "        return 1\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a354f5c-ea55-4487-b006-05d9ddfb9975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_patch(slide, x, y, width, height, level):\n",
    "    \"\"\"\n",
    "    Displays a specified patch from a pathology image file at a given magnification level.\n",
    "\n",
    "    Parameters:\n",
    "    tif_file (str): The path to the TIFF image file.\n",
    "    x (int): The x-coordinate of the top-left corner of the patch.\n",
    "    y (int): The y-coordinate of the top-left corner of the patch.\n",
    "    width (int): The width of the patch.\n",
    "    height (int): The height of the patch.\n",
    "    level (int): The magnification level (0-5, where 5 is the most zoomed-out).\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Scale the coordinates and dimensions to the base level (level 0)\n",
    "    base_x, base_y, base_width, base_height = scaling_images(slide, x, y, width, height, level)\n",
    "\n",
    "    #Check if patch is within bounds\n",
    "    if check_patch_not_within_bounds(slide,base_width, base_height, base_x, base_y):\n",
    "        return\n",
    "    \n",
    "    # Read a region of the slide (patch) at the base level\n",
    "    patch = slide.read_region((base_x, base_y), level, (width, height))\n",
    "\n",
    "    return patch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f70a16ee-8722-4425-bb4b-fd2e0035e99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_bboxes(bboxes, old_size, new_size):\n",
    "    \"\"\"\n",
    "    Scale the bounding boxes accordingly.\n",
    "    \n",
    "    :param bboxes: List of bounding boxes, each in the format [x_min, x_max, y_min, y_max].\n",
    "    :param display_size: Tuple (new_width, new_height) indicating the display size for the image.\n",
    "    :param size_0: Tuple (width, height) indicating the size 0 for the image.\n",
    "    :return: Resized image and scaled bounding boxes.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Scale bounding boxes\n",
    "    x_scale = new_size[0] / old_size[0]\n",
    "    y_scale = new_size[1] / old_size[1]\n",
    "    \n",
    "    scaled_bboxes = []\n",
    "    for bbox in bboxes:\n",
    "        x_min, x_max, y_min, y_max = bbox\n",
    "        new_x_min = int(x_min * x_scale)\n",
    "        new_y_min = int(y_min * y_scale)\n",
    "        new_x_max = int(x_max * x_scale)\n",
    "        new_y_max = int(y_max * y_scale)\n",
    "        scaled_bboxes.append([new_x_min, new_x_max, new_y_min, new_y_max])\n",
    "    \n",
    "    return scaled_bboxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d1fca7a-bda8-4167-b0b3-35f6f15f3556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounding_boxes_for_specific_image(image_name, split='train'):\n",
    "    \"\"\"\n",
    "    prints out the bounding boxes for a specific image file\n",
    "    \"\"\"\n",
    "    #Read in the training data\n",
    "    # Find the row with the specified filename\n",
    "    #split = is_image_train_val_annotated(image_name)\n",
    "    #if split is None:\n",
    "    #    return\n",
    "    data = pd.read_csv(f'./data/{split}.csv')\n",
    "    row = data[data['filename'] == image_name]\n",
    "    row = row.reset_index()\n",
    "    size = (row.loc[0, \"max_x\"], row.loc[0, \"max_y\"])\n",
    "    bboxes = list(row[[\"x1\", \"x2\", \"y1\", \"y2\"]].itertuples(index=False, name=None))\n",
    "    return bboxes,  size[0], size[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4eb14cb-7b16-48af-9e24-43bf00b9c52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_giou_greater(box1, box2):\n",
    "    \"\"\"\n",
    "    Check if two bounding boxes intersect and if the GIoU between them is at least 0.5.\n",
    "    \n",
    "    Parameters:\n",
    "    - box1, box2: Lists or tuples of format [x1_min, y1_min, x1_max, y1_max] where\n",
    "      (x1_min, y1_min) is the top-left coordinate and (x1_max, y1_max) is the bottom-right coordinate.\n",
    "    \n",
    "    Returns:\n",
    "    - True if the GIoU is at least 0.5, False otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpack the coordinates of both bounding boxes\n",
    "    x1_min, x1_max, y1_min, y1_max = box1\n",
    "    x2_min, x2_max, y2_min, y2_max = box2\n",
    "    \n",
    "    # Calculate the area of both bounding boxes\n",
    "    area1 = (x1_max - x1_min) * (y1_max - y1_min)\n",
    "    area2 = (x2_max - x2_min) * (y2_max - y2_min)\n",
    "    \n",
    "    # Calculate the intersection coordinates\n",
    "    inter_x_min = max(x1_min, x2_min)\n",
    "    inter_y_min = max(y1_min, y2_min)\n",
    "    inter_x_max = min(x1_max, x2_max)\n",
    "    inter_y_max = min(y1_max, y2_max)\n",
    "    \n",
    "    # Check if there is an intersection\n",
    "    if inter_x_max < inter_x_min or inter_y_max < inter_y_min:\n",
    "        return False\n",
    "    \n",
    "    # Calculate the area of the intersection\n",
    "    inter_area = (inter_x_max - inter_x_min) * (inter_y_max - inter_y_min)\n",
    "    # Calculate the union area\n",
    "    union_area = area1 + area2 - inter_area\n",
    "    # Calculate IoU\n",
    "    iou = inter_area / union_area\n",
    "    \n",
    "    # Calculate the coordinates of the smallest enclosing box\n",
    "    enc_x_min = min(x1_min, x2_min)\n",
    "    enc_y_min = min(y1_min, y2_min)\n",
    "    enc_x_max = max(x1_max, x2_max)\n",
    "    enc_y_max = max(y1_max, y2_max)\n",
    "    \n",
    "    # Calculate the area of the smallest enclosing box\n",
    "    enc_area = (enc_x_max - enc_x_min) * (enc_y_max - enc_y_min)\n",
    "    \n",
    "    # Calculate GIoU\n",
    "    giou = iou - (enc_area - union_area) / enc_area\n",
    "    \n",
    "    # Return True if GIoU is at least 0.5, otherwise False\n",
    "    return giou #>= 0.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e786b5f-5ac5-431e-babc-1cd12e1aade1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Annotation and label utils\n",
    "\n",
    "def is_image_train_val_annotated(image_name):\n",
    "    data = pd.read_csv('clean_train.csv')\n",
    "    rows = data[data['filename'] == image_name]\n",
    "    if len(rows)>0:\n",
    "        return 'train'\n",
    "    data = pd.read_csv('./data/validation.csv')\n",
    "    rows = data[data['filename'] == image_name]\n",
    "    if len(rows)>0:\n",
    "        return 'validation'\n",
    "    return \n",
    "\n",
    "\n",
    "def is_patch_positive(patch_bbox, ground_truth_bboxes):\n",
    "    max_giou = 0.0\n",
    "    for bbox in ground_truth_bboxes:\n",
    "        new_giou = is_giou_greater(patch_bbox, bbox)\n",
    "        if new_giou > max_giou:\n",
    "            max_giou = new_giou\n",
    "    return max_giou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2e755df-138f-430b-aaaa-4862cc569289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_the_number_of_total_patches(image_path, level, patch_size=200, step_size=512):\n",
    "    slide = openslide.OpenSlide(image_path)\n",
    "    width, height = slide.level_dimensions[level]\n",
    "    count = 0\n",
    "    total_patches_x = (width - patch_size) // step_size + 1\n",
    "    total_patches_y = (height - patch_size) // step_size + 1\n",
    "    total_patches = total_patches_x * total_patches_y\n",
    "    return width,  height\n",
    "\n",
    "def get_width_height(image_path, level):\n",
    "    slide = openslide.OpenSlide(image_path)\n",
    "    width, height = slide.level_dimensions[level]\n",
    "    return width,  height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dd908ee-a51b-4869-b051-e8c4d53176cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_background_patch_ambrosini(patch, od_threshold=0.12):\n",
    "    od = optical_density_transformation_ambrosini(patch)\n",
    "    # Check if any channel has OD less than the threshold\n",
    "    is_background = np.any(od < od_threshold, axis=2)\n",
    "    # Calculate the ratio of background pixels\n",
    "    background_ratio = np.mean(is_background)\n",
    "    return background_ratio > 0.995  # More than 99.5% of the patch is background\n",
    "    \n",
    "def optical_density_transformation_ambrosini(patch):\n",
    "    # Convert patch to numpy array\n",
    "    patch_array = np.array(patch) / 255.0\n",
    "    # Calculate optical density\n",
    "    od = -np.log10(patch_array + 1e-6)\n",
    "    return od\n",
    "\n",
    "def is_patch_white(patch, threshold_white=0.95, brightness_threshold=200):\n",
    "    # Convert patch to grayscale\n",
    "    gray_patch = patch.convert('L')\n",
    "    # Convert to numpy array\n",
    "    gray_array = np.array(gray_patch)\n",
    "    # Calculate the number of white pixels\n",
    "    white_pixels = np.sum(gray_array >= brightness_threshold)\n",
    "    total_pixels = gray_array.size\n",
    "    # Calculate the proportion of white pixels\n",
    "    white_ratio = white_pixels / total_pixels\n",
    "    # Check if the patch is more than threshold_white% white\n",
    "    return white_ratio >= threshold_white\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21721a75-b722-4438-aa20-8c9f490072fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hsv_color_mask(patch):\n",
    "    hsv_patch = cv2.cvtColor(np.array(patch), cv2.COLOR_RGB2HSV)\n",
    "\n",
    "    # Define HSV ranges for blue, purple, and pink colors\n",
    "    lower_blue = np.array([100, 50, 50])\n",
    "    upper_blue = np.array([140, 255, 255])\n",
    "    \n",
    "    lower_purple = np.array([130, 50, 50])\n",
    "    upper_purple = np.array([160, 255, 255])\n",
    "    \n",
    "    lower_pink = np.array([160, 50, 50])\n",
    "    upper_pink = np.array([180, 255, 255])\n",
    "\n",
    "    # Create masks for each color\n",
    "    mask_blue = cv2.inRange(hsv_patch, lower_blue, upper_blue)\n",
    "    mask_purple = cv2.inRange(hsv_patch, lower_purple, upper_purple)\n",
    "    mask_pink = cv2.inRange(hsv_patch, lower_pink, upper_pink)\n",
    "    \n",
    "    # Combine the masks\n",
    "    combined_mask = cv2.bitwise_or(mask_blue, mask_purple)\n",
    "    combined_mask = cv2.bitwise_or(combined_mask, mask_pink)\n",
    "\n",
    "    return combined_mask\n",
    "\n",
    "def calculate_color_coverage(mask):\n",
    "    return np.sum(mask > 0) / mask.size\n",
    "\n",
    "def is_relevant_patch_color(patch, color_threshold=0.05):\n",
    "    mask = hsv_color_mask(patch)\n",
    "    color_coverage = calculate_color_coverage(mask)\n",
    "    return color_coverage >= color_threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3694c6-6646-4c75-8051-c21cd73075e3",
   "metadata": {},
   "source": [
    "### Staining normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86064a0c-5df5-4500-bba0-137b2dd9df55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n## How to call the function:\\nInorm, H, E = normalize_staining(patch)\\n\\n\\n###\\nDefinitions:\\n\\t•\\tNormalized Image (Inorm): Appears similar to the original RGB image but with adjusted colors and intensity\\n        to match a standardized reference. It includes contributions from both Hematoxylin (blue/purple)\\n        and Eosin (pink/red) stains.\\n\\t•\\tHematoxylin Image (H): Primarily shows the nuclei stained by Hematoxylin in shades of blue or purple.\\n        It lacks the pink/red hues contributed by Eosin.\\n    •\\tEosin Image (E): The Eosin image is a single-channel image that represents only the Eosin stain component of the original image. Eosin typically stains the cytoplasm and extracellular matrix in various shades of pink or red.\\n\\n    -> we are interested in either Inorm or H. Eosin is just the pink stuff without the \\'cells\\'.\\n    I would suggest trying out the \\'H\\' because it shows the cells which we are interested in.\\n    \\n\\n\\n##How to display the original and normalized images:\\nfig, ax = plt.subplots(1, 3, figsize=(15, 5))\\n\\n\\nax[0].imshow(image)\\nax[0].set_title(\"Original Image\")\\n\\nax[1].imshow(Inorm)\\nax[1].set_title(\"Normalized Image\")\\n\\n\\nif H is not None:\\n    ax[2].imshow(H)\\n    ax[2].set_title(\"Hematoxylin Image\")\\nelse:\\n    ax[2].imshow(image)\\n    ax[2].set_title(\"No Hematoxylin Image\")\\n\\nplt.show()\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Mitkovetta. (n.d.). normalizeStaining.m. GitHub repository.\n",
    "Retrieved from https://github.com/mitkovetta/staining-normalization/blob/master/normalizeStaining.m\n",
    "[1] A method for normalizing histology slides for quantitative analysis, M Macenko, M Niethammer, JS Marron, D Borland, JT Woosley, G Xiaojun, C Schmitt, NE Thomas, IEEE ISBI, 2009. dx.doi.org/10.1109/ISBI.2009.5193250\n",
    "This MATLAB code was translated to Python with assistance from ChatGPT, developed by OpenAI.\n",
    "\"\"\"\n",
    "def normalize_staining(I, Io=240, beta=0.15, alpha=1, HERef=None, maxCRef=None):\n",
    "    \"\"\"\n",
    "    Normalize the staining appearance of images originating from H&E stained sections.\n",
    "\n",
    "    Parameters:\n",
    "        I (numpy.ndarray): RGB input image.\n",
    "        Io (int, optional): Transmitted light intensity. Default is 240.\n",
    "        beta (float, optional): OD threshold for transparent pixels. Default is 0.15.\n",
    "        alpha (float, optional): Tolerance for the pseudo-min and pseudo-max. Default is 1.\n",
    "        HERef (numpy.ndarray, optional): Reference H&E OD matrix. Default value is defined.\n",
    "        maxCRef (numpy.ndarray, optional): Reference maximum stain concentrations for H&E. Default value is defined.\n",
    "\n",
    "    Returns:\n",
    "        Inorm (PIL.Image.Image): Normalized image.\n",
    "        H (PIL.Image.Image, optional): Hematoxylin image.\n",
    "        E (PIL.Image.Image, optional): Eosin image.\n",
    "    \"\"\"\n",
    "    # Default reference H&E OD matrix\n",
    "    if HERef is None:\n",
    "        HERef = np.array([\n",
    "            [0.5626, 0.2159],\n",
    "            [0.7201, 0.8012],\n",
    "            [0.4062, 0.5581]\n",
    "        ])\n",
    "    # Default reference maximum stain concentrations\n",
    "    if maxCRef is None:\n",
    "        maxCRef = np.array([1.9705, 1.0308])\n",
    "    # Convert PIL image to numpy array if necessary\n",
    "    if isinstance(I, Image.Image):\n",
    "        I = np.array(I)\n",
    "    \n",
    "    h, w, c = I.shape\n",
    "    I = I.reshape((-1, 3)).astype(np.float64)\n",
    "    \n",
    "    # calculate optical density\n",
    "    OD = -np.log((I + 1) / Io)\n",
    "    \n",
    "    # remove transparent pixels\n",
    "    ODhat = OD[np.all(OD >= beta, axis=1)]\n",
    "    \n",
    "    # calculate eigenvectors\n",
    "    _, V = np.linalg.eigh(np.cov(ODhat.T))\n",
    "    \n",
    "    # project on the plane spanned by the eigenvectors corresponding to the two largest eigenvalues\n",
    "    That = np.dot(ODhat, V[:, 1:3])\n",
    "    \n",
    "    # find the min and max vectors and project back to OD space\n",
    "    phi = np.arctan2(That[:, 1], That[:, 0])\n",
    "    \n",
    "    minPhi = scoreatpercentile(phi, alpha)\n",
    "    maxPhi = scoreatpercentile(phi, 100 - alpha)\n",
    "    \n",
    "    vMin = np.dot(V[:, 1:3], [np.cos(minPhi), np.sin(minPhi)])\n",
    "    vMax = np.dot(V[:, 1:3], [np.cos(maxPhi), np.sin(maxPhi)])\n",
    "    # Heuristic to make the vector corresponding to hematoxylin first and eosin second\n",
    "    if vMin[0] > vMax[0]:\n",
    "        HE = np.column_stack((vMin, vMax))\n",
    "    else:\n",
    "        HE = np.column_stack((vMax, vMin))\n",
    "    \n",
    "    Y = OD.T\n",
    "    # Determine concentrations of the individual stains\n",
    "    C = np.linalg.lstsq(HE, Y, rcond=None)[0]\n",
    "    # Normalize stain concentrations\n",
    "    maxC = np.percentile(C, 99, axis=1)\n",
    "    \n",
    "    C = C / maxC[:, np.newaxis]\n",
    "    C = C * maxCRef[:, np.newaxis]\n",
    "    # Recreate the image using reference mixing matrix\n",
    "    Inorm = Io * np.exp(-np.dot(HERef, C))\n",
    "    Inorm = Inorm.T.reshape(h, w, 3)\n",
    "    Inorm = np.clip(Inorm, 0, 255).astype(np.uint8)\n",
    "    Inorm = Image.fromarray(Inorm)\n",
    "    \n",
    "    H, E = None, None\n",
    "    \n",
    "    if C.shape[0] > 1:\n",
    "        H = Io * np.exp(-np.outer(HERef[:, 0], C[0, :]))\n",
    "        H = H.T.reshape(h, w, 3)\n",
    "        H = np.clip(H, 0, 255).astype(np.uint8)\n",
    "        H = Image.fromarray(H)\n",
    "    \n",
    "    if C.shape[0] > 1:\n",
    "        E = Io * np.exp(-np.outer(HERef[:, 1], C[1, :]))\n",
    "        E = E.T.reshape(h, w, 3)\n",
    "        E = np.clip(E, 0, 255).astype(np.uint8)\n",
    "        E = Image.fromarray(E)\n",
    "    \n",
    "    return Inorm, H, E\n",
    "\n",
    "\"\"\"\n",
    "## How to call the function:\n",
    "Inorm, H, E = normalize_staining(patch)\n",
    "\n",
    "\n",
    "###\n",
    "Definitions:\n",
    "\t•\tNormalized Image (Inorm): Appears similar to the original RGB image but with adjusted colors and intensity\n",
    "        to match a standardized reference. It includes contributions from both Hematoxylin (blue/purple)\n",
    "        and Eosin (pink/red) stains.\n",
    "\t•\tHematoxylin Image (H): Primarily shows the nuclei stained by Hematoxylin in shades of blue or purple.\n",
    "        It lacks the pink/red hues contributed by Eosin.\n",
    "    •\tEosin Image (E): The Eosin image is a single-channel image that represents only the Eosin stain component of the original image. Eosin typically stains the cytoplasm and extracellular matrix in various shades of pink or red.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2a0e27d-b9a3-4711-9c78-168c07021e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_colors(image, lower_color_bounds, upper_color_bounds):\n",
    "    \"\"\"\n",
    "    Remove specified color ranges from the image.\n",
    "\n",
    "    Parameters:\n",
    "    image (numpy array): The input image in OpenCV format.\n",
    "    lower_color_bounds (list of numpy arrays): The lower bounds for colors to be removed.\n",
    "    upper_color_bounds (list of numpy arrays): The upper bounds for colors to be removed.\n",
    "\n",
    "    Returns:\n",
    "    numpy array: The image with specified colors removed.\n",
    "    \"\"\"\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    mask = np.zeros(image.shape[:2], dtype=\"uint8\")\n",
    "\n",
    "    for lower, upper in zip(lower_color_bounds, upper_color_bounds):\n",
    "        color_mask = cv2.inRange(hsv, lower, upper)\n",
    "        mask = cv2.bitwise_or(mask, color_mask)\n",
    "\n",
    "    result = cv2.bitwise_and(image, image, mask=cv2.bitwise_not(mask))\n",
    "    return result, mask\n",
    "\n",
    "def highlight_artifacts(original_image, processed_image):\n",
    "    \"\"\"\n",
    "    Highlight artifacts by subtracting processed image from the original image.\n",
    "\n",
    "    Parameters:\n",
    "    original_image (numpy array): The original image.\n",
    "    processed_image (numpy array): The processed image with colors removed.\n",
    "\n",
    "    Returns:\n",
    "    numpy array: The image highlighting the artifacts.\n",
    "    \"\"\"\n",
    "    difference = cv2.absdiff(original_image, processed_image)\n",
    "    _, highlighted = cv2.threshold(difference, 30, 255, cv2.THRESH_BINARY)\n",
    "    return highlighted\n",
    "\n",
    "def remove_artifacts(patch):\n",
    "    \"\"\"\n",
    "    Process a pathology image patch to remove artifacts by:\n",
    "    1. Detecting regions of interest (ROIs) based on color.\n",
    "    2. Highlighting artifacts.\n",
    "    3. Keeping only the ROIs in the original image while making the rest white.\n",
    "\n",
    "    Example Usage:\n",
    "        image_id = '2qj5MlLLBT_a.tif'\n",
    "        image_path = os.path.join('data/images', image_id)\n",
    "        slide = openslide.OpenSlide(image_path)\n",
    "        patch = show_image_patch(slide, 2000, 63, 5000, 2000, 3)\n",
    "        patch = patch.convert('RGB')\n",
    "        patch, highlighted_image, result_image_pil = remove_artifacts(patch)\n",
    "        plot_artifact_removal(patch, highlighted_image, result_image_pil)\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "    patch (PIL Image): The input pathology image patch.\n",
    "\n",
    "    Returns:\n",
    "    tuple: The original patch, highlighted artifacts image, and the final image with ROIs kept and non-ROIs made white.\n",
    "    The patch: 'result_image_pil' is the patch without the artifacts.\n",
    "    \"\"\"\n",
    "    # Convert PIL image to OpenCV format\n",
    "    original_image = cv2.cvtColor(np.array(patch), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Define the range for pink, blue, and purple colors in HSV\n",
    "    lower_bounds = [np.array([120, 20, 70]), np.array([240, 20, 70]), np.array([160, 20, 70])]\n",
    "    upper_bounds = [np.array([180, 255, 255]), np.array([270, 255, 255]), np.array([300, 255, 255])]\n",
    "\n",
    "    # Remove the specified colors\n",
    "    processed_image, mask = remove_colors(original_image, lower_bounds, upper_bounds)\n",
    "\n",
    "    # Highlight the artifacts\n",
    "    highlighted_image = highlight_artifacts(original_image, processed_image)\n",
    "\n",
    "    # Create a mask from the highlighted artifacts\n",
    "    highlighted_image_gray = cv2.cvtColor(highlighted_image, cv2.COLOR_BGR2GRAY)\n",
    "    contours, _ = cv2.findContours(highlighted_image_gray, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Create a mask to keep regions of interest\n",
    "    roi_mask = np.zeros_like(highlighted_image_gray)\n",
    "    cv2.drawContours(roi_mask, contours, -1, (255), thickness=cv2.FILLED)\n",
    "\n",
    "    # Dilate the mask to include some margin around the regions of interest\n",
    "    \"\"\"\n",
    "    1.\tKernel Size Hyperparameter:\n",
    "        •\tThe kernel size used in the dilation process is a critical hyperparameter that affects how much the regions of interest (ROIs) are expanded.\n",
    "        •\tA kernel is a small, square matrix (in our case, 15x15 pixels) that is used to “slide” over the image. Wherever the kernel encounters a white    pixel (part of the ROI), it turns the entire area covered by the kernel white.\n",
    "        •\tThe current kernel size is 15x15 pixels, and we apply it twice (iterations=2). This means that each white pixel in the ROI will expand by 15 pixels in all directions, effectively growing the ROI.\n",
    "    2.\tPurpose of the Kernel:\n",
    "        •\tThe kernel is used to ensure that colors which are not specifically pink, purple, or blue, but are present in the ROIs, are also represented. This is important because the ROIs might contain colors that are close to the specified ones but not exact matches.\n",
    "        •\tBy using a larger kernel, you can increase the margin around the ROIs, ensuring that even slightly different colors in the surrounding area are included in the ROI.\n",
    "    3.\tAdjusting the Kernel Size:\n",
    "        •\tYou can adjust the kernel size to change the margin around the ROIs. A larger kernel will result in a larger margin, meaning the ROIs will expand more.\n",
    "        •\tFor example, if you change the kernel size from 15x15 to 20x20, the dilation process will expand the ROIs by 20 pixels in all directions instead of 15 pixels.\n",
    "    \"\"\"\n",
    "    kernel = np.ones((15, 15), np.uint8)\n",
    "    roi_mask_dilated = cv2.dilate(roi_mask, kernel, iterations=2)\n",
    "\n",
    "    # Apply the mask to the original image\n",
    "    result_image = original_image.copy()\n",
    "    result_image[roi_mask_dilated == 0] = [255, 255, 255]  # Make non-ROI regions white\n",
    "\n",
    "    # Convert back to PIL format for display\n",
    "    result_image_pil = Image.fromarray(cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    return patch, highlighted_image, result_image_pil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508e4f8c-180c-4557-bb4f-82ba4cfceee8",
   "metadata": {},
   "source": [
    "### Remove blurriness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76106df3-8cc4-4a0c-b02d-747fdac1950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_blurriness_and_contrast(image, blur_threshold=70, contrast_threshold=0.35):\n",
    "    # Read the image using OpenCV\n",
    "    if isinstance(image, Image.Image):\n",
    "        image = np.array(image)\n",
    "    # Convert to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Compute the Laplacian of the image and then the variance\n",
    "    laplacian_var = cv2.Laplacian(gray_image, cv2.CV_64F).var()\n",
    "    # Determine if the image is blurry\n",
    "    is_blurry = laplacian_var < blur_threshold\n",
    "    return is_blurry\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69329a67-f954-4be6-8eb5-e5b8bcebb7bc",
   "metadata": {},
   "source": [
    "### Patch selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cc43b7c-76e7-4880-87df-4efb0680ef16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def decide_which_function_for_background(patch, level, final_level = False):\n",
    "    _, _, result_image_pil = remove_artifacts(patch)\n",
    "    if not final_level and level == 4:\n",
    "        return is_patch_white(result_image_pil, 0.65, 254) \n",
    "    if not final_level and level == 3:\n",
    "        return is_patch_white(result_image_pil, 0.40, 254)  or detect_blurriness_and_contrast(result_image_pil)\n",
    "    if final_level:\n",
    "        return is_patch_white(result_image_pil, 0.15, 254)  or detect_blurriness_and_contrast(result_image_pil)\n",
    "    return is_patch_white(result_image_pil, 0.8, 254) \n",
    "\n",
    "\n",
    "def do_pyramid_level(slide, image_name, width, height, final_level, current_level, base_x, base_y, search_width, search_height, patch_size, step_size, split, patches_info, ground_truth_bboxes):\n",
    "    is_final_level = (final_level == current_level)\n",
    "    \n",
    "    if not is_final_level:\n",
    "        current_step_size = step_size \n",
    "    else:\n",
    "\n",
    "        current_step_size = step_size\n",
    "    for y in range(base_y, base_y + search_height - patch_size + current_step_size, current_step_size):\n",
    "        if y > base_y + search_height - patch_size:\n",
    "            patch_y = base_y + search_height - patch_size\n",
    "        else:\n",
    "            patch_y = y\n",
    "        for x in range(base_x, base_x + search_width - patch_size + current_step_size, current_step_size):    \n",
    "            if x > base_x + search_width - patch_size:\n",
    "                patch_x = base_x + search_width - patch_size\n",
    "            else:\n",
    "                patch_x = x\n",
    "           \n",
    "            patch = show_image_patch(slide, patch_x, patch_y, patch_size, patch_size, current_level)\n",
    "            patch = patch.convert(\"RGB\")\n",
    "            \n",
    "            # Check if the patch is background\n",
    "            if not decide_which_function_for_background(patch, current_level, is_final_level):\n",
    "                if is_final_level:\n",
    "                    patch_bbox = (patch_x, patch_x + patch_size, patch_y, patch_y + patch_size)\n",
    "                    label = is_patch_positive(patch_bbox, ground_truth_bboxes)\n",
    "                    patches_info['label'].append(int(label))\n",
    "                    patches_info['x1'].append(patch_x)\n",
    "                    patches_info['y1'].append(patch_y)\n",
    "                    patches_info['filename'].append(image_name)\n",
    "                    patches_info['image_width'].append(width)\n",
    "                    patches_info['image_height'].append(height)\n",
    "                else:\n",
    "                    new_base_x, new_base_y = patch_x * 2, patch_y*2 \n",
    "                    patches_info = do_pyramid_level(slide, image_name, width, height, final_level, current_level-1, new_base_x, new_base_y, patch_size*2, patch_size*2, patch_size, step_size, split, patches_info, ground_truth_bboxes)\n",
    "            \n",
    "    if current_level == 5:\n",
    "        tqdm_pbar_images.update(1)\n",
    "    return patches_info\n",
    "        \n",
    "\n",
    "def do_pyramid(image_path, level=1, patch_size=224, step_size=64):\n",
    "\n",
    "    # Open the whole slide image\n",
    "    # Open the TIFF file\n",
    "    slide = openslide.OpenSlide(image_path)\n",
    "    width, height = slide.level_dimensions[level]\n",
    "    split = is_image_train_val_annotated(image_path.split('/')[-1])\n",
    "    image_name = image_path.split('/')[-1]\n",
    "    if split == 'train':\n",
    "        ground_truth_bboxes, old_width, old_height= get_bounding_boxes_for_specific_image(image_path.split('/')[-1])\n",
    "        ground_truth_bboxes = resize_bboxes(ground_truth_bboxes, (old_width, old_height), (width, height))\n",
    "    else:\n",
    "        return []\n",
    "   \n",
    "    level_5_width, level_5_height = slide.level_dimensions[5]\n",
    "    patches_info = {'label': [], 'x1':[], 'y1':[], 'filename':[], 'image_width':[], 'image_height':[]}\n",
    "\n",
    "    return do_pyramid_level(slide, image_name, width, height, level, 5, 0, 0, level_5_width, level_5_height, patch_size, step_size, split, patches_info, ground_truth_bboxes)\n",
    "\n",
    "def process_image(image_path,  level, patch_size, step_size):\n",
    "\n",
    "    \n",
    "    try:\n",
    "        patches_info = do_pyramid(image_path, patch_size=patch_size, step_size=step_size, level=level)\n",
    "        return patches_info\n",
    "    except Exception as e:\n",
    "        # Handle the error by returning a failure status\n",
    "        print(e, image_path)\n",
    "        return []\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a50721",
   "metadata": {},
   "source": [
    "## Create CSV file with suitable patch coordinates\n",
    "The following section of the code generates a CSV file containing the coordinates of the suitable patches for all images in the image folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc89fc92-39b6-4d13-88c8-665f521d0bc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 224 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   0%|          | 3/4415 [10:11<251:17:05, 205.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsupported or missing image file ./data/images/AK7tmBa937_b.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   0%|          | 12/4415 [32:04<222:29:40, 181.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsupported or missing image file ./data/images/o26ULhMO2U_b.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   0%|          | 0/4415 [48:30<?, ?it/s]1, 205.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label     x1    y1          filename image_width image_height\n",
      "0     0  18200  4600  jlrFhKAJs0_a.tif       20736        49472\n",
      "1     0  18224  4600  jlrFhKAJs0_a.tif       20736        49472\n",
      "2     0  18200  4624  jlrFhKAJs0_a.tif       20736        49472\n",
      "3     0  18224  4624  jlrFhKAJs0_a.tif       20736        49472\n",
      "4     0  18048  4600  jlrFhKAJs0_a.tif       20736        49472\n",
      "(2169585, 6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get list of image files in the folder\n",
    "image_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))] #[:50]\n",
    "\n",
    "# Shared DataFrame (empty initially)\n",
    "columns = ['label', 'x1',  'y1',  'filename', 'image_width', 'image_height']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "def init(pbar_images):\n",
    "    global tqdm_pbar_images\n",
    "    tqdm_pbar_images = pbar_images\n",
    "\n",
    "# Create a multiprocessing pool\n",
    "# Create a tqdm progress bar\n",
    "with tqdm(total=len(image_files), desc='Processing images') as pbar_images:\n",
    "    with Pool(processes=16, initializer=init, initargs=(pbar_images, )) as pool:  # Adjust the number of processes as needed\n",
    "        # Map the function to process each image path\n",
    "        results = []\n",
    "        for image_path in image_files:\n",
    "                results.append(pool.apply_async(process_image, (image_path, level, patch_size, step_size)))\n",
    "        \n",
    "        # Retrieve results from processes\n",
    "        for result in results:\n",
    "            result = pd.DataFrame(result.get())\n",
    "            df = df.append(result, ignore_index=True)\n",
    "df = df.drop_duplicates()\n",
    "# Print the final DataFrame with processed results\n",
    "df.to_csv(csv_file, index=False)\n",
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc73558b-9717-465f-b0f8-2e19d72f1e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  index          filename     x1     x2     y1     y2  max_x  \\\n",
      "0           0      0  bGaslniO4a_a.tif  29348  30108  28404  29675  82944   \n",
      "1           1      1  bGaslniO4a_a.tif  11735  12379  70274  71195  82944   \n",
      "2           2      2  2qj5MlLLBT_a.tif  11185  12276  11571  12671  82944   \n",
      "3           3      3  2qj5MlLLBT_a.tif  14380  15583  11252  12434  82944   \n",
      "4           4      4  2qj5MlLLBT_a.tif  12162  13834  71136  72440  82944   \n",
      "\n",
      "    max_y  \n",
      "0  197632  \n",
      "1  197632  \n",
      "2  196608  \n",
      "3  196608  \n",
      "4  196608  \n",
      "   label     x1    y1          filename  image_width  image_height\n",
      "0      0  18200  4600  jlrFhKAJs0_a.tif        20736         49472\n",
      "1      0  18224  4600  jlrFhKAJs0_a.tif        20736         49472\n",
      "2      0  18200  4624  jlrFhKAJs0_a.tif        20736         49472\n",
      "3      0  18224  4624  jlrFhKAJs0_a.tif        20736         49472\n",
      "4      0  18048  4600  jlrFhKAJs0_a.tif        20736         49472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "910it [04:04,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_coverage(bbox_gt, bbox_patch, bbox_gt_area):\n",
    "    \"\"\"\n",
    "    Compute the coverage of bounding box 1 by bounding box 2.\n",
    "    \n",
    "    Parameters:\n",
    "    - box1, box2: Lists or tuples of format [x1, y1, x2, y2] where\n",
    "      (x1, y1) is the top-left coordinate and (x2, y2) is the bottom-right coordinate.\n",
    "    \n",
    "    Returns:\n",
    "    - coverage: The proportion of box 1 covered by box 2.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpack the coordinates of both bounding boxes\n",
    "    x1_min, x1_max, y1_min, y1_max = bbox_gt\n",
    "    x2_min, x2_max, y2_min, y2_max = bbox_patch\n",
    "    \n",
    "    # Calculate the (x, y)-coordinates of the intersection rectangle\n",
    "    inter_x_min = max(x1_min, x2_min)\n",
    "    inter_y_min = max(y1_min, y2_min)\n",
    "    inter_x_max = min(x1_max, x2_max)\n",
    "    inter_y_max = min(y1_max, y2_max)\n",
    "    \n",
    "    # Compute the area of the intersection rectangle\n",
    "    inter_width = max(0, inter_x_max - inter_x_min)\n",
    "    inter_height = max(0, inter_y_max - inter_y_min)\n",
    "    inter_area = inter_width * inter_height\n",
    "    \n",
    "    # Compute the coverage\n",
    "    coverage = inter_area / bbox_gt_area if bbox_gt_area != 0 else 0\n",
    "    \n",
    "    return coverage\n",
    "\n",
    "def are_extracted_patches_valid(patches_path, annotation_path, threshold=0.75):\n",
    "    patches_df = pd.read_csv(patches_path)\n",
    "    ann_df = pd.read_csv(annotation_path)\n",
    "    print(ann_df.head())\n",
    "\n",
    "    print(patches_df.head())\n",
    "    \n",
    "    missing = []\n",
    "    count = 0\n",
    "    for _, image_row in tqdm(ann_df.iterrows()):\n",
    "        bbox_gt = image_row[\"x1\"], image_row[\"x2\"], image_row[\"y1\"], image_row[\"y2\"]\n",
    "\n",
    "        coverage = 0\n",
    "        filename = image_row['filename']\n",
    "        old_width, old_height = image_row['max_x'], image_row['max_y']\n",
    "\n",
    "        image_patches = patches_df[patches_df['filename'] == filename]\n",
    "        \n",
    "        image_patches =image_patches.reset_index()\n",
    "        if image_patches.shape[0] == 0:\n",
    "            missing.append(filename)\n",
    "        else:\n",
    "            width, height= image_patches.iloc[0]['image_width'], image_patches.iloc[0]['image_height']\n",
    "\n",
    "            bbox_gt = resize_bboxes([bbox_gt], (old_width, old_height), (width, height))[0]\n",
    "            bbox_gt_area = (bbox_gt[1] - bbox_gt[0]) * (bbox_gt[3]- bbox_gt[2])\n",
    "            for _, patch_row in image_patches.iterrows():\n",
    "                bbox_patch = patch_row[\"x1\"], patch_row[\"x1\"]+224, patch_row[\"y1\"], patch_row[\"y1\"]+224\n",
    "                patch_coverage = compute_coverage(bbox_gt, bbox_patch, bbox_gt_area)\n",
    "                coverage += patch_coverage\n",
    "            if  coverage < threshold:\n",
    "                count += 1\n",
    "    return count, missing\n",
    "\n",
    "\n",
    "annotation_path_train = 'clean_train.csv'\n",
    "\n",
    "count, missing = are_extracted_patches_valid(patches_path, annotation_path_train, threshold=0.9)\n",
    "print(count, len(missing))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aab41fc",
   "metadata": {},
   "source": [
    "The following code reads the CSV file containing the patch coordinates, splits the data into training and validation sets based on IDs from new_val_ids.txt, balances the training set by oversampling patches that are parts of a lesion, and saves the processed training and validation sets to separate CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e6c97611-99ad-4f34-baf7-c7e9e229bd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2169585, 6)\n",
      "247\n",
      "['kiAdmoDDMM_a.tif', 'Mn1A3CnXrc_b.tif', '0w9NQUKyFU_b.tif', 'fgh7blkYnD_b.tif', 'StKznQnYkK_a.tif', '4y4EEMpL4L_a.tif', '51nZ7GBlix_b.tif', 'lYkGPxzccV_a.tif']\n",
      "8\n",
      "(2106665, 6)\n",
      "(62920, 6)\n",
      "4898\n",
      "(14694, 6)\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "\n",
    "patches_df = pd.read_csv(patches_path)\n",
    "print(patches_df.shape)\n",
    "\n",
    "train_csv_file = os.path.join(csv_dir, 'train.csv')\n",
    "val_csv_file = os.path.join(csv_dir, 'corrected_val.csv')\n",
    "\n",
    "if not os.path.exists(csv_dir):\n",
    "    os.makedirs(csv_dir)\n",
    "\n",
    "image_ids = patches_df['filename'].unique()\n",
    "print(len(image_ids))\n",
    "val_ids = list(train_test_split(image_ids, test_size=0.03, shuffle=True, random_state=42)[1])\n",
    "print(val_ids)\n",
    "\n",
    "\n",
    "print(len(val_ids))\n",
    "with open('new_val_ids.txt', mode='w') as file:\n",
    "    for number in val_ids:\n",
    "        file.write(f\"{number}\\n\")\n",
    "\n",
    "train_df = patches_df[patches_df['filename'].isin(val_ids) == False].reset_index(drop=True)\n",
    "val_df = patches_df[patches_df['filename'].isin(val_ids)].reset_index(drop=True)\n",
    "\n",
    "print(train_df.shape)\n",
    "print(val_df.shape)\n",
    "\n",
    "\n",
    "train_label_1_df = train_df[train_df['label'] == 1]\n",
    "n = len(train_label_1_df)\n",
    "print(n)\n",
    "\n",
    "train_label_0_df = train_df[train_df['label'] == 0].sample(n=2*n, random_state=seed)\n",
    "\n",
    "all_train_df_sampled = pd.concat([train_label_0_df, train_label_1_df])\n",
    "\n",
    "print(all_train_df_sampled.shape)\n",
    "\n",
    "all_train_df_sampled.to_csv(train_csv_file, index=False)\n",
    "val_df.to_csv(val_csv_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f37fed99-56a4-4b02-9c74-cd26626ebd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77614, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 247/247 [17:57<00:00,  4.36s/it]  \n"
     ]
    }
   ],
   "source": [
    "def extract_patches_from_images(data_path, csv_folder, zip_name, patch_size=224, level=0):\n",
    "    # Open the whole slide image\n",
    "    train_df = pd.read_csv(os.path.join(csv_folder, 'train.csv'))\n",
    "    val_df = pd.read_csv(os.path.join(csv_folder, 'corrected_val.csv'))\n",
    "    all_df = pd.concat([train_df, val_df])\n",
    "    print(all_df.shape)\n",
    "    grouped = all_df.groupby('filename')\n",
    "    dfs_by_imagename = [group for _, group in grouped]\n",
    "    with zipfile.ZipFile(zip_name, 'w') as zipf:\n",
    "        for df in tqdm(dfs_by_imagename):\n",
    "            df = df.reset_index()\n",
    "            file_name =  df.loc[0, 'filename']\n",
    "            slide_path = os.path.join(data_path, file_name)\n",
    "            slide = openslide.OpenSlide(slide_path)\n",
    "            \n",
    "            width, height = slide.dimensions\n",
    "\n",
    "            for _, patch_row in  df.iterrows():\n",
    "                # Get the patch\n",
    "                x, y = patch_row['x1'], patch_row['y1']\n",
    "                base_x, base_y, _, _ = scaling_images(slide, x, y, patch_size, patch_size, level)\n",
    "                patch = slide.read_region((base_x, base_y), level, (patch_size, patch_size)).convert('RGB')\n",
    "                _, _, patch = remove_artifacts(patch)\n",
    "                try:\n",
    "                    with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as temp_file:\n",
    "                        temp_path = temp_file.name\n",
    "                        patch.save(temp_path, format='PNG')\n",
    "                        zipf.write(temp_path, os.path.basename(f\"{file_name.split('.')[0]}_{x}_{y}_{patch_size}.png\"))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing file {file_name}: {e}\")\n",
    "\n",
    "\n",
    "extract_patches_from_images(folder_path, csv_dir, zip_name, patch_size, level=level)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
